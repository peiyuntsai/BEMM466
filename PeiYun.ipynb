{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# Reddit API \n",
    "reddit = praw.Reddit(\n",
    "    client_id='PiX5OOgasCP_mtmS2JjUrg',  # client_id\n",
    "    client_secret='Yjc8QGuRvTsX_sPyJj92RwkBMM0xpA',  # client_secret\n",
    "    user_agent='stacy5067'  # user_agent\n",
    ")\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "subreddit_name = 'electricvehicles'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "with open('reddit_posts.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'Title', 'Content', 'Date'])\n",
    "\n",
    "    post_count = 0\n",
    "    num_posts= 1000\n",
    "\n",
    "    for submission in subreddit.new(limit=num_posts):\n",
    "        post_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        writer.writerow([submission.id, submission.title, submission.selftext, post_date])\n",
    "        post_count += 1\n",
    "\n",
    "print(f\"Data has been saved to 'reddit_posts.csv'. Total posts: {post_count}\")\n",
    "\n",
    "subreddit_name = 'electriccars'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "with open('relectriccars.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'Title', 'Content', 'Date'])\n",
    "\n",
    "    for submission in subreddit.new(limit=num_posts):\n",
    "        post_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')  \n",
    "        writer.writerow([submission.id, submission.title, submission.selftext, post_date])\n",
    "        post_count += 1  \n",
    "\n",
    "print(f\"Data has been saved to 'relectriccars.csv'. Total posts: {post_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"reddit_posts.csv\")\n",
    "df2 = pd.read_csv(\"relectriccars.csv\")\n",
    "\n",
    "df_merged = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df_merged.to_csv(\"merged_reddit_posts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('merged_reddit_posts.csv')\n",
    "data['Text'] = data['Title'] + ' ' + data['Content'].fillna('')  \n",
    "data.drop_duplicates(subset='ID', keep='first', inplace=True)\n",
    "data = data[~data['Text'].str.contains(\"weekly roundup for week of\")]\n",
    "text_lengths = data['Text'].apply(len)  \n",
    "text_stats = text_lengths.describe()  \n",
    "\n",
    "print(\"Descriptive statistics for text lengthï¼š\")\n",
    "print(text_stats)\n",
    "\n",
    "num_texts = len(data['Text'])\n",
    "print(f\"There are a total of {num_texts} texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "data['Cleaned_Text'] = data['Text'].apply(clean_text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = ['ev', 'vehicle', 'car', 'im', 'wa', 'ha', 'dont', 'u', 'doe',\n",
    "                     'ive', 'id', 'v', 'doesnt', 'cant', 'hi', 'isnt', 'k', 'wont', 'le', \n",
    "                     'e', 'r', 'wouldnt', 'youre', 'x', 'hello', 'couldnt', 'arent', \n",
    "                     'wasnt', 'hey', 'l', 'f', 'q', 'st', 'lol', 'n', 'c', 'th', \n",
    "                     'would', 'could', 'didnt', 'j', 'b', 'w', 'etc', 'whats',\n",
    "                     'one', 'two', 'three', 'weve', 'ok', 'okay','ea','mi','ex','h', 'p', 'g','na', 'thanks',\n",
    "                     'also', 'may', 'thats', 'might','done','thank', 'must','need','people']\n",
    "\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n",
    "    return filtered_words\n",
    "data['Processed_Text'] = data['Cleaned_Text'].apply(tokenize_and_lemmatize)\n",
    "dictionary = corpora.Dictionary(data['Processed_Text'])\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.35, keep_n=10000)\n",
    "corpus = [dictionary.doc2bow(text) for text in data['Processed_Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_word_list = [word for text in data['Cleaned_Text'] for word in text.split()]\n",
    "cleaned_word_list = [word for text in data['Processed_Text'] for word in text]\n",
    "\n",
    "# Display the first 100 words before cleaning\n",
    "print(\"First 100 words before cleaning:\", original_word_list[:100])\n",
    "\n",
    "# Display the first 100 words after cleaning\n",
    "print(\"First 100 words after cleaning:\", cleaned_word_list[:100])\n",
    "\n",
    "# Display the total number of words before and after cleaning\n",
    "print(\"Total number of words before cleaning:\", len(original_word_list))\n",
    "print(\"Total number of words after cleaning:\", len(cleaned_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display text before and after cleaning\n",
    "def display_original_and_processed(text):\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    # Tokenize and lemmatize\n",
    "    processed_words = tokenize_and_lemmatize(cleaned_text)\n",
    "    # Display results\n",
    "    print(\"Original text:\", text)\n",
    "    print(\"Cleaned text:\", cleaned_text)\n",
    "    print(\"Text after tokenization and lemmatization:\", processed_words)\n",
    "\n",
    "# Example text\n",
    "example_text = data['Text'].iloc[290]\n",
    "display_original_and_processed(example_text)\n",
    "\n",
    "word_freq = sorted(dictionary.dfs.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "for word_id, freq in word_freq:\n",
    "    print(f'Word: {dictionary[word_id]}, Frequency: {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_freq_dict = {dictionary[word_id]: freq for word_id, freq in word_freq}\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    max_words=300,            \n",
    "    min_font_size=10,         \n",
    "    scale=3,                  \n",
    "    stopwords=stop_words      \n",
    ").generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step):\n",
    "    coherence_values = []\n",
    "    perplexity_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=20, random_state=42)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "        perplexity_values.append(np.exp(model.log_perplexity(corpus)))\n",
    "    return model_list, coherence_values, perplexity_values\n",
    "\n",
    "start, limit, step = 2, 11, 1  \n",
    "model_list, coherence_values, perplexity_values = compute_coherence_values(dictionary, corpus, data['Processed_Text'], start, limit, step)\n",
    "\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, coherence_values, marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Topic Coherence Metrics\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, perplexity_values, marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Perplexity Score\")\n",
    "plt.title(\"Topic Perplexity Metrics\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "start = 2  \n",
    "\n",
    "index = num_topics - start \n",
    "\n",
    "coherence_score = coherence_values[index]\n",
    "perplexity_score = perplexity_values[index]\n",
    "\n",
    "print(f\"Coherence Score at {num_topics} topics: {coherence_score}\")\n",
    "print(f\"Perplexity Score at {num_topics} topics: {perplexity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final LDA model\n",
    "final_lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Print each topic and its keywords\n",
    "for idx, topic in final_lda_model.print_topics(num_words=20):\n",
    "    print(f'Topic: {idx} \\nKeywords: {topic}')\n",
    "\n",
    "# Analyze the main topic of each document\n",
    "topic_counts = [0] * num_topics  # Initialize a counter based on the number of topics\n",
    "for doc_bow in corpus:\n",
    "    topics = final_lda_model.get_document_topics(doc_bow)\n",
    "    # Calculate the main topic index for each document (no need to adjust the index)\n",
    "    main_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # Direct counting\n",
    "    topic_counts[main_topic] += 1\n",
    "\n",
    "# Calculate the total number of documents and the proportion of each topic\n",
    "total_docs = len(corpus)\n",
    "topic_proportions = [count / total_docs for count in topic_counts]\n",
    "\n",
    "print(\"Topic Counts:\", topic_counts)\n",
    "print(\"Topic Proportions:\", topic_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "for idx, topic in final_lda_model.show_topics(formatted=False, num_words=30):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False).generate_from_frequencies(dict(topic))\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic {}\".format(idx))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = [final_lda_model.get_document_topics(doc_bow) for doc_bow in corpus]\n",
    "target_topic_id = 0\n",
    "relevant_docs = [\n",
    "    (idx, topic_prob) for idx, topics in enumerate(document_topics)\n",
    "    for topic_id, topic_prob in topics\n",
    "    if topic_id == target_topic_id and topic_prob >= 0.1\n",
    "]\n",
    "\n",
    "relevant_docs_sorted = sorted(relevant_docs, key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "for doc_idx, topic_prob in relevant_docs_sorted:\n",
    "    print(f\"\\nDocument Index: {doc_idx}\")\n",
    "    print(f\"Topic Probability: {topic_prob}\")\n",
    "    print(f\"Original Text: {data['Text'].iloc[doc_idx]}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = [final_lda_model.get_document_topics(doc_bow) for doc_bow in corpus]\n",
    "target_topic_id = 1\n",
    "relevant_docs = [\n",
    "    (idx, topic_prob) for idx, topics in enumerate(document_topics)\n",
    "    for topic_id, topic_prob in topics\n",
    "    if topic_id == target_topic_id and topic_prob >= 0.1\n",
    "]\n",
    "\n",
    "relevant_docs_sorted = sorted(relevant_docs, key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "for doc_idx, topic_prob in relevant_docs_sorted:\n",
    "    print(f\"\\nDocument Index: {doc_idx}\")\n",
    "    print(f\"Topic Probability: {topic_prob}\")\n",
    "    print(f\"Original Text: {data['Text'].iloc[doc_idx]}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = [final_lda_model.get_document_topics(doc_bow) for doc_bow in corpus]\n",
    "target_topic_id = 2\n",
    "relevant_docs = [\n",
    "    (idx, topic_prob) for idx, topics in enumerate(document_topics)\n",
    "    for topic_id, topic_prob in topics\n",
    "    if topic_id == target_topic_id and topic_prob >= 0.1\n",
    "]\n",
    "\n",
    "relevant_docs_sorted = sorted(relevant_docs, key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "for doc_idx, topic_prob in relevant_docs_sorted:\n",
    "    print(f\"\\nDocument Index: {doc_idx}\")\n",
    "    print(f\"Topic Probability: {topic_prob}\")\n",
    "    print(f\"Original Text: {data['Text'].iloc[doc_idx]}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = [final_lda_model.get_document_topics(doc_bow) for doc_bow in corpus]\n",
    "target_topic_id = 3\n",
    "relevant_docs = [\n",
    "    (idx, topic_prob) for idx, topics in enumerate(document_topics)\n",
    "    for topic_id, topic_prob in topics\n",
    "    if topic_id == target_topic_id and topic_prob >= 0.1\n",
    "]\n",
    "\n",
    "relevant_docs_sorted = sorted(relevant_docs, key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "for doc_idx, topic_prob in relevant_docs_sorted:\n",
    "    print(f\"\\nDocument Index: {doc_idx}\")\n",
    "    print(f\"Topic Probability: {topic_prob}\")\n",
    "    print(f\"Original Text: {data['Text'].iloc[doc_idx]}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = [final_lda_model.get_document_topics(doc_bow) for doc_bow in corpus]\n",
    "target_topic_id = 4\n",
    "relevant_docs = [\n",
    "    (idx, topic_prob) for idx, topics in enumerate(document_topics)\n",
    "    for topic_id, topic_prob in topics\n",
    "    if topic_id == target_topic_id and topic_prob >= 0.1\n",
    "]\n",
    "\n",
    "relevant_docs_sorted = sorted(relevant_docs, key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "for doc_idx, topic_prob in relevant_docs_sorted:\n",
    "    print(f\"\\nDocument Index: {doc_idx}\")\n",
    "    print(f\"Topic Probability: {topic_prob}\")\n",
    "    print(f\"Original Text: {data['Text'].iloc[doc_idx]}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "daily_counts = data.resample('D').size()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(daily_counts.index, daily_counts.values, color='skyblue')\n",
    "plt.title('Daily Posts Count')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Posts')\n",
    "\n",
    "\n",
    "ax = plt.gca()  \n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))  \n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  \n",
    "\n",
    "\n",
    "start_date = pd.to_datetime('2024-05-07')\n",
    "end_date = pd.to_datetime('2024-07-18')\n",
    "ax.set_xlim(start_date, end_date)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts = data.resample('D').size()\n",
    "average_posts = daily_counts.mean()\n",
    "median_posts = daily_counts.median()\n",
    "max_posts = daily_counts.max()\n",
    "min_posts = daily_counts.min()\n",
    "std_dev_posts = daily_counts.std()\n",
    "\n",
    "print(f\"Average number of daily posts: {average_posts}\")\n",
    "print(f\"Median number of daily posts: {median_posts}\")\n",
    "print(f\"Maximum number of daily posts: {max_posts}\")\n",
    "print(f\"Minimum number of daily posts: {min_posts}\")\n",
    "print(f\"Standard deviation of daily posts: {std_dev_posts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from gensim.models import LdaModel\n",
    "import numpy as np\n",
    "\n",
    "# Operate on each topic\n",
    "for topic_id in range(5):  \n",
    "    # Extract the top 20 keywords for this topic\n",
    "    topic_keywords = [word for word, _ in final_lda_model.show_topic(topic_id, topn=20)]\n",
    "    \n",
    "    # Initialize a co-occurrence network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for keyword in topic_keywords:\n",
    "        G.add_node(keyword)\n",
    "    \n",
    "    # Compute co-occurrence within the documents\n",
    "    for document in data['Processed_Text']:\n",
    "        present_keywords = [word for word in topic_keywords if word in document]\n",
    "        for word1, word2 in combinations(present_keywords, 2):\n",
    "            if G.has_edge(word1, word2):\n",
    "                G[word1][word2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(word1, word2, weight=1)\n",
    "\n",
    "    # Draw the graph using a Spring layout\n",
    "    pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "    \n",
    "    # Prepare edge weights and colors for drawing\n",
    "    edges, weights = zip(*nx.get_edge_attributes(G, 'weight').items())\n",
    "    norm = plt.Normalize(min(weights), max(weights))\n",
    "    edge_colors = [plt.cm.Blues(norm(weight)) for weight in weights]\n",
    "\n",
    "    # Draw the network graph\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    nx.draw(G, pos, ax=ax, node_color='skyblue', node_size=50, edgelist=edges,\n",
    "            edge_color=edge_colors, width=2, with_labels=True, font_size=10, font_color='black', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'keyword Co-occurrence Network within Topic {topic_id}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store centrality statistics\n",
    "centrality_stats = pd.DataFrame(columns=['Topic', 'Average Degree Centrality', 'SD Degree Centrality', 'Average Closeness Centrality', 'SD Closeness Centrality'])\n",
    "\n",
    "# Operate on each topic\n",
    "for topic_id in range(5):  # Assuming there are 5 topics\n",
    "    # Extract the top 30 keywords for this topic\n",
    "    topic_keywords = [word for word, _ in final_lda_model.show_topic(topic_id, topn=30)]\n",
    "    \n",
    "    # Initialize a co-occurrence network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for keyword in topic_keywords:\n",
    "        G.add_node(keyword)\n",
    "    \n",
    "    # Compute co-occurrence within the documents\n",
    "    for document in data['Processed_Text']:\n",
    "        present_keywords = [word for word in topic_keywords if word in document]\n",
    "        for word1, word2 in combinations(present_keywords, 2):\n",
    "            if G.has_edge(word1, word2):\n",
    "                G[word1][word2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(word1, word2, weight=1)\n",
    "\n",
    "    # Calculate degree centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    centrality_values = np.array(list(degree_centrality.values()))\n",
    "    avg_degree_centrality = np.mean(centrality_values)\n",
    "    sd_degree_centrality = np.std(centrality_values)\n",
    "\n",
    "    # Calculate closeness centrality\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    closeness_values = np.array(list(closeness_centrality.values()))\n",
    "    avg_closeness_centrality = np.mean(closeness_values)\n",
    "    sd_closeness_centrality = np.std(closeness_values)\n",
    "\n",
    "    # Store statistics in the DataFrame\n",
    "    centrality_stats.loc[topic_id] = [topic_id, avg_degree_centrality, sd_degree_centrality, avg_closeness_centrality, sd_closeness_centrality]\n",
    "\n",
    "    # Print the results for each topic\n",
    "    print(f\"Network characteristics for Topic {topic_id}:\")\n",
    "    print(f\"Average Degree Centrality: {avg_degree_centrality:.4f}\")\n",
    "    print(f\"Standard Deviation of Degree Centrality: {sd_degree_centrality:.4f}\")\n",
    "    print(f\"Average Closeness Centrality: {avg_closeness_centrality:.4f}\")\n",
    "    print(f\"Standard Deviation of Closeness Centrality: {sd_closeness_centrality:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
